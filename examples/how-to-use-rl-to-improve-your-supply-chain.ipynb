{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "937597e4",
   "metadata": {},
   "source": [
    "# How to Use Deep Reinforcement Learning to Improve your Supply Chain\n",
    "\n",
    "Full write up available [here](https://www.datahubbs.com/how-to-use-deep-reinforcement-learning-to-improve-your-supply-chain/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fefefc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import or_gym\n",
    "from or_gym.utils import create_env\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray import tune\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40fa580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_env(env_name, env_config={}):\n",
    "    env = create_env(env_name)\n",
    "    tune.register_env(env_name, \n",
    "        lambda env_name: env(env_name,\n",
    "            env_config=env_config))\n",
    "\n",
    "# Environment and RL Configuration Settings\n",
    "env_name = 'InvManagement-v1'\n",
    "# env_name = \"Knapsack-v0\"\n",
    "env_config = {} # Change environment parameters here\n",
    "rl_config = dict(\n",
    "    env=env_name,\n",
    "    num_workers=2,\n",
    "    env_config=env_config,\n",
    "    model=dict(\n",
    "        vf_share_layers=False,\n",
    "        fcnet_activation='elu',\n",
    "        fcnet_hiddens=[256, 256]\n",
    "    ),\n",
    "    lr=1e-5\n",
    ")\n",
    " \n",
    "# Register environment\n",
    "register_env(env_name, env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea13304f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=32512)\u001b[0m 2022-08-17 16:32:57,950\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32513)\u001b[0m 2022-08-17 16:32:57,936\tWARNING env.py:135 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32512)\u001b[0m 2022-08-17 16:32:58,260\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=32512, ip=192.168.1.177, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f220d2cf9a0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32512)\u001b[0m   File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32512)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32512)\u001b[0m   File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32512)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32512)\u001b[0m   File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/policy/policy_map.py\", line 140, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32512)\u001b[0m     self[policy_id] = class_(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32512)\u001b[0m   File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/policy/tf_policy_template.py\", line 256, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32512)\u001b[0m     DynamicTFPolicy.__init__(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32512)\u001b[0m   File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/policy/dynamic_tf_policy.py\", line 387, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32512)\u001b[0m     ) = self.exploration.get_exploration_action(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32512)\u001b[0m   File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/utils/exploration/stochastic_sampling.py\", line 82, in get_exploration_action\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32512)\u001b[0m     return self._get_tf_exploration_action_op(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32512)\u001b[0m   File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/utils/exploration/stochastic_sampling.py\", line 89, in _get_tf_exploration_action_op\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32512)\u001b[0m     stochastic_actions = tf.cond(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32512)\u001b[0m   File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32512)\u001b[0m     raise e.with_traceback(filtered_tb) from None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32512)\u001b[0m   File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/utils/exploration/stochastic_sampling.py\", line 92, in <lambda>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32512)\u001b[0m     self.random_exploration.get_tf_exploration_action_op(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32512)\u001b[0m   File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/utils/exploration/random.py\", line 136, in get_tf_exploration_action_op\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32512)\u001b[0m     action = tf.cond(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32512)\u001b[0m   File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/utils/exploration/random.py\", line 65, in true_fn\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32512)\u001b[0m     action_dist.required_model_output_shape(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32512)\u001b[0m   File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/models/tf/tf_action_dist.py\", line 187, in required_model_output_shape\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32512)\u001b[0m     assert np.all(action_space.high == high_)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32512)\u001b[0m AssertionError\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32513)\u001b[0m 2022-08-17 16:32:58,229\tERROR worker.py:451 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=32513, ip=192.168.1.177, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f1cf9ddf940>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32513)\u001b[0m   File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32513)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32513)\u001b[0m   File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32513)\u001b[0m     self.policy_map.create_policy(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32513)\u001b[0m   File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/policy/policy_map.py\", line 140, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32513)\u001b[0m     self[policy_id] = class_(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32513)\u001b[0m   File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/policy/tf_policy_template.py\", line 256, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32513)\u001b[0m     DynamicTFPolicy.__init__(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32513)\u001b[0m   File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/policy/dynamic_tf_policy.py\", line 387, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32513)\u001b[0m     ) = self.exploration.get_exploration_action(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32513)\u001b[0m   File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/utils/exploration/stochastic_sampling.py\", line 82, in get_exploration_action\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32513)\u001b[0m     return self._get_tf_exploration_action_op(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32513)\u001b[0m   File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/utils/exploration/stochastic_sampling.py\", line 89, in _get_tf_exploration_action_op\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32513)\u001b[0m     stochastic_actions = tf.cond(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32513)\u001b[0m   File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32513)\u001b[0m     raise e.with_traceback(filtered_tb) from None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32513)\u001b[0m   File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/utils/exploration/stochastic_sampling.py\", line 92, in <lambda>\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32513)\u001b[0m     self.random_exploration.get_tf_exploration_action_op(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32513)\u001b[0m   File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/utils/exploration/random.py\", line 136, in get_tf_exploration_action_op\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32513)\u001b[0m     action = tf.cond(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32513)\u001b[0m   File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/utils/exploration/random.py\", line 65, in true_fn\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32513)\u001b[0m     action_dist.required_model_output_shape(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32513)\u001b[0m   File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/models/tf/tf_action_dist.py\", line 187, in required_model_output_shape\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32513)\u001b[0m     assert np.all(action_space.high == high_)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=32513)\u001b[0m AssertionError\n"
     ]
    },
    {
     "ename": "RayActorError",
     "evalue": "The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=32512, ip=192.168.1.177, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f220d2cf9a0>)\n  File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n    self._build_policy_map(\n  File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n    self.policy_map.create_policy(\n  File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/policy/policy_map.py\", line 140, in create_policy\n    self[policy_id] = class_(\n  File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/policy/tf_policy_template.py\", line 256, in __init__\n    DynamicTFPolicy.__init__(\n  File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/policy/dynamic_tf_policy.py\", line 387, in __init__\n    ) = self.exploration.get_exploration_action(\n  File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/utils/exploration/stochastic_sampling.py\", line 82, in get_exploration_action\n    return self._get_tf_exploration_action_op(\n  File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/utils/exploration/stochastic_sampling.py\", line 89, in _get_tf_exploration_action_op\n    stochastic_actions = tf.cond(\n  File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/utils/exploration/stochastic_sampling.py\", line 92, in <lambda>\n    self.random_exploration.get_tf_exploration_action_op(\n  File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/utils/exploration/random.py\", line 136, in get_tf_exploration_action_op\n    action = tf.cond(\n  File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/utils/exploration/random.py\", line 65, in true_fn\n    action_dist.required_model_output_shape(\n  File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/models/tf/tf_action_dist.py\", line 187, in required_model_output_shape\n    assert np.all(action_space.high == high_)\nAssertionError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/agents/trainer.py:935\u001b[0m, in \u001b[0;36mTrainer.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 935\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_creator)\n\u001b[1;32m    936\u001b[0m \u001b[39m# New design: Override `Trainable.setup()` (as indented by Trainable)\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[39m# and do or don't call super().setup() from within your override.\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[39m# By default, `super().setup()` will create both worker sets:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[39m# parallel to training.\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \u001b[39m# TODO: Deprecate `_init()` and remove this try/except block.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/agents/trainer.py:1074\u001b[0m, in \u001b[0;36mTrainer._init\u001b[0;34m(self, config, env_creator)\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_init\u001b[39m(\u001b[39mself\u001b[39m, config: TrainerConfigDict, env_creator: EnvCreator) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1074\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRayActorError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/home/christian/GitHub/or-gym/examples/how-to-use-rl-to-improve-your-supply-chain.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/christian/GitHub/or-gym/examples/how-to-use-rl-to-improve-your-supply-chain.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Initialize Ray and Build Agent\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/christian/GitHub/or-gym/examples/how-to-use-rl-to-improve-your-supply-chain.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m ray\u001b[39m.\u001b[39minit(ignore_reinit_error\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/christian/GitHub/or-gym/examples/how-to-use-rl-to-improve-your-supply-chain.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m agent \u001b[39m=\u001b[39m PPOTrainer(env\u001b[39m=\u001b[39;49menv_name,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/christian/GitHub/or-gym/examples/how-to-use-rl-to-improve-your-supply-chain.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     config\u001b[39m=\u001b[39;49mrl_config)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/christian/GitHub/or-gym/examples/how-to-use-rl-to-improve-your-supply-chain.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m results \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/christian/GitHub/or-gym/examples/how-to-use-rl-to-improve-your-supply-chain.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m500\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/agents/trainer.py:870\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, config, env, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[39m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \u001b[39m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[39m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[39m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    862\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_metrics \u001b[39m=\u001b[39m {\n\u001b[1;32m    863\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mevaluation\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[1;32m    864\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mepisode_reward_max\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mnan,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    867\u001b[0m     }\n\u001b[1;32m    868\u001b[0m }\n\u001b[0;32m--> 870\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    871\u001b[0m     config, logger_creator, remote_checkpoint_dir, sync_function_tpl\n\u001b[1;32m    872\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/tune/trainable.py:156\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001b[0m\n\u001b[1;32m    154\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    155\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_ip \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_current_ip()\n\u001b[0;32m--> 156\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup(copy\u001b[39m.\u001b[39;49mdeepcopy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig))\n\u001b[1;32m    157\u001b[0m setup_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[1;32m    158\u001b[0m \u001b[39mif\u001b[39;00m setup_time \u001b[39m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m~/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/agents/trainer.py:950\u001b[0m, in \u001b[0;36mTrainer.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[39m# New design: Override `Trainable.setup()` (as indented by Trainable)\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[39m# and do or don't call super().setup() from within your override.\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[39m# By default, `super().setup()` will create both worker sets:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[39m# parallel to training.\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \u001b[39m# TODO: Deprecate `_init()` and remove this try/except block.\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    944\u001b[0m     \u001b[39m# Only if user did not override `_init()`:\u001b[39;00m\n\u001b[1;32m    945\u001b[0m     \u001b[39m# - Create rollout workers here automatically.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[39m# This matches the behavior of using `build_trainer()`, which\u001b[39;00m\n\u001b[1;32m    949\u001b[0m     \u001b[39m# has been deprecated.\u001b[39;00m\n\u001b[0;32m--> 950\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers \u001b[39m=\u001b[39m WorkerSet(\n\u001b[1;32m    951\u001b[0m         env_creator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_creator,\n\u001b[1;32m    952\u001b[0m         validate_env\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalidate_env,\n\u001b[1;32m    953\u001b[0m         policy_class\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_default_policy_class(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig),\n\u001b[1;32m    954\u001b[0m         trainer_config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig,\n\u001b[1;32m    955\u001b[0m         num_workers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mnum_workers\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    956\u001b[0m         local_worker\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    957\u001b[0m         logdir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogdir,\n\u001b[1;32m    958\u001b[0m     )\n\u001b[1;32m    959\u001b[0m     \u001b[39m# By default, collect metrics for all remote workers.\u001b[39;00m\n\u001b[1;32m    960\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_remote_workers_for_metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers\u001b[39m.\u001b[39mremote_workers()\n",
      "File \u001b[0;32m~/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py:142\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, policy_class, trainer_config, num_workers, local_worker, logdir, _setup)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39m# Create a local worker, if needed.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[39m# If num_workers > 0 and we don't have an env on the local worker,\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39m# get the observation- and action spaces for each policy from\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39m# the first remote worker (which does have an env).\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    134\u001b[0m     local_worker\n\u001b[1;32m    135\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_remote_workers\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m     )\n\u001b[1;32m    141\u001b[0m ):\n\u001b[0;32m--> 142\u001b[0m     remote_spaces \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39;49mget(\n\u001b[1;32m    143\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mremote_workers()[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mforeach_policy\u001b[39m.\u001b[39;49mremote(\n\u001b[1;32m    144\u001b[0m             \u001b[39mlambda\u001b[39;49;00m p, pid: (pid, p\u001b[39m.\u001b[39;49mobservation_space, p\u001b[39m.\u001b[39;49maction_space)\n\u001b[1;32m    145\u001b[0m         )\n\u001b[1;32m    146\u001b[0m     )\n\u001b[1;32m    147\u001b[0m     spaces \u001b[39m=\u001b[39m {\n\u001b[1;32m    148\u001b[0m         e[\u001b[39m0\u001b[39m]: (\u001b[39mgetattr\u001b[39m(e[\u001b[39m1\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39moriginal_space\u001b[39m\u001b[39m\"\u001b[39m, e[\u001b[39m1\u001b[39m]), e[\u001b[39m2\u001b[39m])\n\u001b[1;32m    149\u001b[0m         \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m remote_spaces\n\u001b[1;32m    150\u001b[0m     }\n\u001b[1;32m    151\u001b[0m     \u001b[39m# Try to add the actual env's obs/action spaces.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[39mif\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minit\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(ray, func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/worker.py:1833\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   1831\u001b[0m             \u001b[39mraise\u001b[39;00m value\u001b[39m.\u001b[39mas_instanceof_cause()\n\u001b[1;32m   1832\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1833\u001b[0m             \u001b[39mraise\u001b[39;00m value\n\u001b[1;32m   1835\u001b[0m \u001b[39mif\u001b[39;00m is_individual_id:\n\u001b[1;32m   1836\u001b[0m     values \u001b[39m=\u001b[39m values[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mRayActorError\u001b[0m: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=32512, ip=192.168.1.177, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f220d2cf9a0>)\n  File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 630, in __init__\n    self._build_policy_map(\n  File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1788, in _build_policy_map\n    self.policy_map.create_policy(\n  File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/policy/policy_map.py\", line 140, in create_policy\n    self[policy_id] = class_(\n  File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/policy/tf_policy_template.py\", line 256, in __init__\n    DynamicTFPolicy.__init__(\n  File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/policy/dynamic_tf_policy.py\", line 387, in __init__\n    ) = self.exploration.get_exploration_action(\n  File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/utils/exploration/stochastic_sampling.py\", line 82, in get_exploration_action\n    return self._get_tf_exploration_action_op(\n  File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/utils/exploration/stochastic_sampling.py\", line 89, in _get_tf_exploration_action_op\n    stochastic_actions = tf.cond(\n  File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/utils/exploration/stochastic_sampling.py\", line 92, in <lambda>\n    self.random_exploration.get_tf_exploration_action_op(\n  File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/utils/exploration/random.py\", line 136, in get_tf_exploration_action_op\n    action = tf.cond(\n  File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/utils/exploration/random.py\", line 65, in true_fn\n    action_dist.required_model_output_shape(\n  File \"/home/christian/anaconda3/envs/or-gym/lib/python3.9/site-packages/ray/rllib/models/tf/tf_action_dist.py\", line 187, in required_model_output_shape\n    assert np.all(action_space.high == high_)\nAssertionError"
     ]
    }
   ],
   "source": [
    "# Initialize Ray and Build Agent\n",
    "ray.init(ignore_reinit_error=True)\n",
    "agent = PPOTrainer(env=env_name,\n",
    "    config=rl_config)\n",
    " \n",
    "results = []\n",
    "for i in range(500):\n",
    "    res = agent.train()\n",
    "    results.append(res)\n",
    "    if (i+1) % 5 == 0:\n",
    "        print('\\rIter: {}\\tReward: {:.2f}'.format(\n",
    "                i+1, res['episode_reward_mean']), end='')\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "793e41cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([0 0 0], [100  90  80], (3,), int16)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = or_gym.make(\"InvManagement-v0\")\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44bb7398",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/christian/GitHub/or-gym/examples/how-to-use-rl-to-improve-your-supply-chain.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/christian/GitHub/or-gym/examples/how-to-use-rl-to-improve-your-supply-chain.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m high_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmax(env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mhigh)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/christian/GitHub/or-gym/examples/how-to-use-rl-to-improve-your-supply-chain.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39mall(env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mlow \u001b[39m==\u001b[39m low_)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/christian/GitHub/or-gym/examples/how-to-use-rl-to-improve-your-supply-chain.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39mall(env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mhigh \u001b[39m==\u001b[39m high_)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "low_ = np.min(env.action_space.low)\n",
    "high_ = np.max(env.action_space.high)\n",
    "\n",
    "assert np.all(env.action_space.low == low_)\n",
    "assert np.all(env.action_space.high == high_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "741bbd80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, array([100,  90,  80], dtype=int16))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_, env.action_space.high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bdaf2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100,  90,  80], dtype=int16)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    def required_model_output_shape(\n",
    "        action_space: gym.Space, model_config: ModelConfigDict\n",
    "    ) -> Union[int, np.ndarray]:\n",
    "        # Int Box.\n",
    "        if isinstance(action_space, gym.spaces.Box):\n",
    "            assert action_space.dtype.name.startswith(\"int\")\n",
    "            low_ = np.min(action_space.low)\n",
    "            high_ = np.max(action_space.high)\n",
    "            assert np.all(action_space.low == low_)\n",
    "            assert np.all(action_space.high == high_)\n",
    "            np.prod(action_space.shape, dtype=np.int32) * (high_ - low_ + 1)\n",
    "        # MultiDiscrete space.\n",
    "        else:\n",
    "            # nvec is already integer, so no casting needed.\n",
    "            return np.sum(action_space.nvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acdf3907",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Box' object has no attribute 'nvec'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/christian/GitHub/or-gym/examples/how-to-use-rl-to-improve-your-supply-chain.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/christian/GitHub/or-gym/examples/how-to-use-rl-to-improve-your-supply-chain.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m env\u001b[39m.\u001b[39;49maction_space\u001b[39m.\u001b[39;49mnvec\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Box' object has no attribute 'nvec'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "env.action_space.nvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b29ab14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def5147b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('or-gym')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "7a3c55ba1ddbbe9183e589d8fd627c7965373c0721772952510236acf25b6085"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
